# Setup Next Steps

## âœ… Completed

### Trust Pages Created
- `/pages/about.html` â€” Editorial independence & mission
- `/pages/privacy.html` â€” Data handling & cookies
- `/pages/disclosure.html` â€” FTC affiliate disclosure (critical!)
- `/pages/contact.html` â€” Email contact + takedown requests
- `robots.txt` â€” Search engine crawling rules

### Documentation
- `AUTOMATION.md` â€” Full guide to affiliate automation

---

## ðŸŽ¥ Optional: Video Upload Setup

**Note:** Video upload to YouTube is completely optional. The site works perfectly without it!

### Prerequisites
- YouTube channel
- Google Cloud account (free tier is fine)
- OpenAI account (pay-as-you-go)
- Pexels account (free)

### Step-by-Step YouTube OAuth Setup

#### 1. Create Google Cloud Project
1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project (e.g., "ForgeFinds Video Upload")
3. Enable **YouTube Data API v3**:
   - In the left menu: APIs & Services â†’ Library
   - Search for "YouTube Data API v3"
   - Click Enable

#### 2. Configure OAuth Consent Screen
1. Go to: APIs & Services â†’ OAuth consent screen
2. Choose "External" user type
3. Fill in required fields:
   - App name: ForgeFinds
   - User support email: your email
   - Developer contact: your email
4. Add scope: `https://www.googleapis.com/auth/youtube.upload`
5. Add yourself as a test user
6. Save and continue

#### 3. Create OAuth Credentials
1. Go to: APIs & Services â†’ Credentials
2. Click "Create Credentials" â†’ "OAuth client ID"
3. Choose "Desktop app" as application type
4. Name it "ForgeFinds Desktop"
5. Click Create
6. **Save the Client ID and Client Secret** â€” you'll need these

#### 4. Generate Refresh Token
```bash
# Add Client ID and Secret to .env first
echo "YT_CLIENT_ID=your_client_id_here" >> .env
echo "YT_CLIENT_SECRET=your_client_secret_here" >> .env

# Run the token generation script
npm run youtube:refresh-token
```

Follow the browser prompt to authorize access. Copy the refresh token shown in the terminal.

#### 5. Get Your Channel ID
1. Go to [YouTube Studio](https://studio.youtube.com)
2. Settings â†’ Channel â†’ Advanced settings
3. Copy your Channel ID (starts with `UC...`)

#### 6. Configure All Credentials

**Local setup (.env file):**
```bash
# YouTube OAuth
YT_CLIENT_ID=your_client_id_here
YT_CLIENT_SECRET=your_client_secret_here
YT_REFRESH_TOKEN=your_refresh_token_here
YT_CHANNEL_ID=UCxxxxxxxxxxxxx

# OpenAI (get from platform.openai.com/api-keys)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini

# Pexels (get from pexels.com/api)
PEXELS_API_KEY=xxxxxxxxxxxxxxxx

# Logo path
FORGEFINDS_LOGO_PATH=./forgefinds-logo.png
```

**GitHub Actions (Repository Secrets):**
1. Go to: Repository â†’ Settings â†’ Secrets and variables â†’ Actions
2. Add each secret individually:
   - `YT_CLIENT_ID`
   - `YT_CLIENT_SECRET`
   - `YT_REFRESH_TOKEN`
   - `YT_CHANNEL_ID`
   - `OPENAI_API_KEY`
   - `PEXELS_API_KEY`

#### 7. Validate Setup
```bash
npm run validate
```

This checks which credentials are configured and provides setup links for missing ones.

### Troubleshooting YouTube Authentication

**Error: `invalid_grant`**

This means your refresh token is expired or invalid. Common causes:
- Token expired after 6 months of inactivity
- OAuth consent was revoked
- Client credentials changed

**Fix:**
```bash
npm run youtube:refresh-token
```
Then update the `YT_REFRESH_TOKEN` secret in GitHub Actions.

**Error: Missing credentials**

Run the validation script to see what's missing:
```bash
npm run validate
```

**Workflow skipping video upload:**

This is normal if credentials aren't configured. The workflow will:
- Check for required secrets
- Skip gracefully if any are missing
- Display which credentials need to be configured
- Continue working for daily deals scraping

---

## ðŸš€ What Schema.org Does (Simple)

**Schema.org = Invisible labels that tell Google "this is a real product deal"**

It prevents Google from flagging your site as suspicious because:
1. Google can now see structured pricing/product data
2. Looks legitimate, not like affiliate spam
3. Improves search ranking visibility

**We'll add it to each deal card automatically** â€” no manual work needed.

---

## ðŸ“‹ Your To-Do List

### Week 1: Affiliate Setup
- [ ] Sign up for **Amazon Associates** (if not already)
- [ ] Get your **Tracking ID** (format: `yourname-20`)
- [ ] Decide: **Option A (API)** or **Option B (Web Scraping)**
  - Option A = slower setup (2-4 weeks), more reliable long-term
  - Option B = works immediately, simpler to start

### Week 2: Automation
- [ ] Create `.github/workflows/daily-scrape.yml`
- [ ] Create `scraper/update-deals.js`
- [ ] Create scraper file (web-scraper.js or amazon-scraper.js)
- [ ] Add GitHub Secrets: `AMAZON_TRACKING_ID`
- [ ] Test locally: `node scraper/update-deals.js`
- [ ] Push to GitHub â†’ automates daily!

### Week 3: Schema.org + Launch
- [ ] Update `assets/deals.js` with schema.org markup (JSON-LD)
- [ ] Test Google Search Console
- [ ] Submit sitemap to Google
- [ ] Monitor for "suspicious site" flags (should disappear)

---

## ðŸ“¦ Files We Created

```
/pages/
  â”œâ”€â”€ about.html          # Who you are, mission, no bias
  â”œâ”€â”€ privacy.html        # Data handling, Google Analytics
  â”œâ”€â”€ disclosure.html     # "We earn affiliate $$" (legal required!)
  â””â”€â”€ contact.html        # Email contact + feedback
  
robots.txt               # Tells Google to crawl you
AUTOMATION.md            # Full scraper setup guide
```

---

## ðŸ’¡ Why This Matters for Google

**Before (Risky):**
- Generic "deals" page
- Affiliate links everywhere
- No disclosure
- No structured data
- â†’ Google flags as suspicious affiliate spam

**After (Legitimate):**
- Clear "About" page explaining editorial independence
- Transparent affiliate disclosure (FTC-compliant)
- Schema.org markup showing real product data
- Contact + privacy info
- â†’ Google sees professional operation

---

## ðŸŽ¯ Final Architecture

```
ForgeFinds (Static Site)
    â†“
GitHub Repo
    â†“
GitHub Actions (Daily 6 AM UTC)
    â†“
Scraper (Web or API) â†’ Searches Amazon for deals
    â†“
Data transforms + affiliate links added
    â†“
Updates deals.json automatically
    â†“
Site loads from deals.json every day
    â†“
Schema.org markup tells Google these are real offers
    â†“
Better rankings, no "suspicious site" flag âœ…
```

---

## ðŸ¤” Questions?

- **What if I don't want to automate yet?** â€” Just manually edit `data/deals.json` using the current format. Pages + schema.org still help with legitimacy.
- **How many deals can I have?** â€” Depends on scraper. Start with 50-100, scale to 500+.
- **Will this get me in trouble with Amazon?** â€” No, if you're using official APIs or legitimate scraping. Just disclose (âœ… you now do).

Next step: Pick **Option A or B** and let me know if you want help setting up the scraper!
